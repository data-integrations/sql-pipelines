/*
 * Copyright Â© 2019 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.pipeline.sql.app.core;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.reflect.TypeToken;
import io.cdap.cdap.api.customaction.AbstractCustomAction;
import io.cdap.cdap.etl.planner.Dag;
import io.cdap.cdap.etl.proto.Connection;
import io.cdap.cdap.etl.proto.v2.ETLStage;
import io.cdap.pipeline.sql.api.template.SQLSink;
import io.cdap.pipeline.sql.api.template.SQLSource;
import io.cdap.pipeline.sql.api.template.SQLTransform;
import io.cdap.pipeline.sql.api.template.tables.AbstractTableInfo;
import org.apache.calcite.jdbc.CalciteSchema;
import org.apache.calcite.jdbc.Driver;
import org.apache.calcite.rel.RelNode;
import org.apache.calcite.rel.rel2sql.RelToSqlConverter;
import org.apache.calcite.rel.type.RelDataTypeField;
import org.apache.calcite.schema.ColumnStrategy;
import org.apache.calcite.schema.SchemaPlus;
import org.apache.calcite.sql.SqlDataTypeSpec;
import org.apache.calcite.sql.SqlDialect;
import org.apache.calcite.sql.SqlIdentifier;
import org.apache.calcite.sql.SqlNode;
import org.apache.calcite.sql.SqlNodeList;
import org.apache.calcite.sql.ddl.SqlCreateTable;
import org.apache.calcite.sql.ddl.SqlDdlNodes;
import org.apache.calcite.sql.parser.SqlParserPos;
import org.apache.calcite.sql.type.SqlTypeUtil;
import org.apache.calcite.sql.util.SqlBuilder;
import org.apache.calcite.tools.FrameworkConfig;
import org.apache.calcite.tools.Frameworks;
import org.apache.calcite.tools.RelBuilder;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

/**
 * A custom action which executes SQL upon a target platform.
 */
public abstract class AbstractSQLExecutor extends AbstractCustomAction {
  private static final String TEMPORARY_TABLE_PREFIX = "temporary_table_";
  private static final String CONFIG_NAME = "serializedSqlConfig";
  private static final String FROM_CONNECTIONS_NAME = "fromConnections";
  private static final String TO_CONNECTIONS_NAME = "toConnections";
  private static final String SOURCES_NAME = "sources";
  private static final String SINKS_NAME = "sinks";
  private static final String CREATE_TABLE_FEATURE_FLAG = "createTable";

  private SQLConfig config;
  private Dag dag;
  private Set<String> sourceStages;
  private Set<String> sinkStages;
  private Map<String, SQLTransform> pluginMap;
  private Map<String, Set<String>> fromNodeMap;
  private Map<String, Set<String>> toNodeMap;
  private Map<String, RelNode> relationalNodeMap;
  private Map<String, TemporaryTable> temporaryTableMap;
  private List<String> queries;
  private boolean createTableFeatureFlag;

  public AbstractSQLExecutor(SQLConfig config, Map<String, SQLTransform> pluginMap) {
    this.config = config;
    this.sourceStages = new HashSet<>();
    this.sinkStages = new HashSet<>();
    this.pluginMap = pluginMap;
    this.fromNodeMap = new HashMap<>();
    this.toNodeMap = new HashMap<>();
    this.relationalNodeMap = new HashMap<>();
    this.temporaryTableMap = new HashMap<>();
    this.queries = new ArrayList<>();
    this.createTableFeatureFlag = false;
  }

  /**
   * Returns the list of queries which were generated by the planner. init() must be called prior to
   * retrieving the queries, otherwise this could return an empty list.
   *
   * @return The generated query plan
   */
  public List<String> getQueries() {
    return queries;
  }

  /**
   * Returns the {@link SqlDialect} to use for the concrete implementation of this class.
   *
   * @return The dialect for the concrete implementation
   */
  public abstract SqlDialect getDialect();

  @Override
  public void configure() {
    Gson gson = new GsonBuilder().create();

    // Property map for storing runtime state
    Map<String, String> properties = new HashMap<>();
    // Initialize the executor state and store in property map
    initExecutorConfig(gson, properties);

    // Load the Calcite JDBC driver
    loadCalciteJdbcDriver();
    // Traverse the topological ordering to prepare the queries for validation
    traverseTopologicalOrder();
  }

  @Override
  public void initialize() {
    // Set the config
    Gson gson = new GsonBuilder().create();

    // Flag to enable or disable creation of tables when they don't exist
    createTableFeatureFlag = false;
    // Set whether we want to create tables if they do not exist
    if (getContext().getRuntimeArguments().get(CREATE_TABLE_FEATURE_FLAG).equals("true")) {
      createTableFeatureFlag = true;
    }

    // Load the dag and other properties previously initialized at configure time
    loadExecutorConfig(gson, getContext().getSpecification().getProperties());

    // Load the Calcite JDBC driver
    loadCalciteJdbcDriver();
    // Traversal Logic to prepare the query ordering
    traverseTopologicalOrder();

    // Serialize all sink and splitter RelNodes
    // Converter to convert RelNodes to SqlNodes
    RelToSqlConverter converter = new RelToSqlConverter(getDialect());
    // Creates a temporary table for each splitter
    createTemporaryTables(converter);
    // Creates a query for each sink
    createSinkQueries(converter);
    // Make the query list unmodifiable
    queries = Collections.unmodifiableList(queries);
  }

  /**
   * Initializes the config of the executor and stores the GSON of the state in the properties map. This should
   * only happen at configure time.
   */
  private void initExecutorConfig(Gson gson, Map<String, String> properties) {
    // Store the config
    properties.put(CONFIG_NAME, gson.toJson(config));

    // Initialize the dag
    if (!config.getConnections().isEmpty()) {
      dag = new Dag(config.getConnections());
    } else {
      dag = null;
    }

    // Set the connections maps
    for (Connection connection: config.getConnections()) {
      if (!fromNodeMap.containsKey(connection.getTo())) {
        fromNodeMap.put(connection.getTo(), new HashSet<>());
      }
      if (!toNodeMap.containsKey(connection.getFrom())) {
        toNodeMap.put(connection.getFrom(), new HashSet<>());
      }
      fromNodeMap.get(connection.getTo()).add(connection.getFrom());
      toNodeMap.get(connection.getFrom()).add(connection.getTo());
    }
    // Save the connections maps
    properties.put(FROM_CONNECTIONS_NAME, gson.toJson(fromNodeMap));
    properties.put(TO_CONNECTIONS_NAME, gson.toJson(toNodeMap));

    // Set sources and sinks
    for (ETLStage stage: config.getStages()) {
      if (!fromNodeMap.containsKey(stage.getName())) {
        sourceStages.add(stage.getName());
      }
      if (!toNodeMap.containsKey(stage.getName())) {
        sinkStages.add(stage.getName());
      }
    }
    // Save the sources and sinks
    properties.put(SOURCES_NAME, gson.toJson(sourceStages));
    properties.put(SINKS_NAME, gson.toJson(sinkStages));

    // Store the config JSON for access during runtime
    setProperties(properties);
  }

  /**
   * Loads the executor config property GSONs from the property map. This should only happen at runtime.
   */
  private void loadExecutorConfig(Gson gson, Map<String, String> properties) {
    // Initialize the instance variables
    sourceStages = new HashSet<>();
    sinkStages = new HashSet<>();
    pluginMap = new HashMap<>();
    fromNodeMap = new HashMap<>();
    toNodeMap = new HashMap<>();
    relationalNodeMap = new HashMap<>();
    temporaryTableMap = new HashMap<>();
    queries = new ArrayList<>();

    // Load the config
    config = gson.fromJson(getContext().getSpecification().getProperty(CONFIG_NAME), SQLConfig.class);
    // Load the plugins
    for (ETLStage stage: config.getStages()) {
      String node = stage.getName();

      // Instantiate the plugin
      SQLTransform plugin;
      try {
        plugin = getContext().newPluginInstance(node);
      } catch (InstantiationException e) {
        // This probably should have been caught at configure time
        throw new IllegalStateException("Failed to instantiate plugin for stage " + node);
      }
      pluginMap.put(node, plugin);
    }
    // Initialize the dag
    if (!config.getConnections().isEmpty()) {
      dag = new Dag(config.getConnections());
    } else {
      dag = null;
    }
    // Load connections maps
    fromNodeMap = gson.fromJson(properties.get(FROM_CONNECTIONS_NAME),
                                new TypeToken<Map<String, Set<String>>>() { }.getType());
    toNodeMap = gson.fromJson(properties.get(TO_CONNECTIONS_NAME),
                              new TypeToken<Map<String, Set<String>>>() { }.getType());
    // Load the sources and sinks
    sourceStages = gson.fromJson(properties.get(SOURCES_NAME), sourceStages.getClass());
    sinkStages = gson.fromJson(properties.get(SINKS_NAME), sinkStages.getClass());
  }

  /**
   * Helper method which loads the Calcite JDBC driver from {@link Driver}.
   */
  private void loadCalciteJdbcDriver() {
    // Load the Calcite JDBC driver
    try {
      Class.forName(Driver.class.getName());
    } catch (ClassNotFoundException e) {
      // This should not happen assuming this app is properly packaged
      throw new IllegalStateException("Unable to load the Calcite JDBC driver.");
    }
  }

  /**
   * The actions to perform while iterating through all pipeline nodes in topological order.
   */
  private void traverseTopologicalOrder() {
    // Get topological ordering
    List<String> topologicalOrder;
    if (dag != null) {
      topologicalOrder = dag.getTopologicalOrder();
    } else {
      topologicalOrder = new ArrayList<>();
    }

    // Schema and framework config creation
    SchemaPlus rootSchema = CalciteSchema.createRootSchema(true).plus();
    final FrameworkConfig builderConfig = Frameworks.newConfigBuilder().defaultSchema(rootSchema).build();
    // Identifier for temporary table
    int temporaryTableCounter = 0;
    // Traverse the topological ordering
    for (String node: topologicalOrder) {
      // Get the plugin
      SQLTransform plugin = pluginMap.get(node);

      // Create the RelBuilder for generating the query
      final RelBuilder builder = RelBuilder.create(builderConfig);

      // Generate schema from source nodes
      // Adding the tables directly to the SchemaPlus seems kind of hacky
      // TODO: CDAP-16095 Revisit schema generation
      if (sourceStages.contains(node)) {
        // The stage is a source node
        SQLSource sourcePlugin = (SQLSource) plugin;
        AbstractTableInfo table = sourcePlugin.getSourceTable();
        // Add the table to the root schema
        rootSchema.add(table.getTableName(), table);
        // Add a scan for the source table
        builder.scan(table.getTableName());
      } else {
        // Node is not a source node
        // Add all dependencies of the current node to the builder parameter stack
        Set<String> fromNodes = fromNodeMap.get(node);
        for (String fromNode : fromNodes) {
          // Check if there exists a temporary table
          if (temporaryTableMap.containsKey(fromNode)) {
            // Scan the temporary table
            builder.scan(temporaryTableMap.get(fromNode).getTableName());
          } else {
            // Otherwise push the inlined query
            builder.push(relationalNodeMap.get(fromNode));
          }
        }

        // Pass the builder to the node to allow it to combine the inputs
        int dependencies = fromNodes.size();
        plugin.combineInputs(builder, dependencies);
      }

      // Get the relational query node and set the node in the map
      relationalNodeMap.put(node, plugin.getQuery(builder));

      // Check if the node is a splitter node
      if (toNodeMap.containsKey(node) && toNodeMap.get(node).size() > 1) {
        // Add a temporary table if splitter
        String temporaryTableName = TEMPORARY_TABLE_PREFIX + temporaryTableCounter++;
        // This adds the potential temporary table schema to the root schema
        TemporaryTable temporaryTable = new TemporaryTable(temporaryTableName,
                                                           relationalNodeMap.get(node).getRowType());
        temporaryTableMap.put(node, temporaryTable);
        rootSchema.add(temporaryTableName, temporaryTable);
      }
    }
  }

  /**
   * Generates a single temporary table from each node with multiple outputs, sourcing from source or other
   * splitter nodes.
   *
   * @param converter The converter to convert the {@link RelNode} to a {@link SqlNode}
   */
  private void createTemporaryTables(RelToSqlConverter converter) {
    // Prepending the INSERT INTO and CREATE TEMPORARY statements seem rather hacky
    // TODO: CDAP-16096 Revisit generation of the CREATE TEMPORARY and INSERT INTO statement for sink stages
    for (String node: temporaryTableMap.keySet()) {
      RelNode relationalQueryNode = relationalNodeMap.get(node);
      SqlNode sqlQueryNode = converter.visitChild(0, relationalQueryNode).asQueryOrValues();
      SqlBuilder query = new SqlBuilder(getDialect());
      query.append("CREATE TEMPORARY TABLE ");
      query.identifier(temporaryTableMap.get(node).getTableName());
      query.append(" AS ");
      query.append(sqlQueryNode.toSqlString(getDialect()));
      queries.add(query.toSqlString().getSql());
    }
  }

  /**
   * Generates a single complex query for each and every sink, sourcing from source or splitter nodes.
   *
   * @param converter The converter to convert the {@link RelNode} to a {@link SqlNode}
   */
  private void createSinkQueries(RelToSqlConverter converter) {
    for (String node: sinkStages) {
      SQLSink sinkPlugin = (SQLSink) pluginMap.get(node);
      AbstractTableInfo destinationTable = sinkPlugin.getDestinationTable();
      RelNode relationalQueryNode = relationalNodeMap.get(node);

      if (createTableFeatureFlag) {
        // Build a create table statement for each sink stage
        SqlNodeList columns = new SqlNodeList(SqlParserPos.ZERO);
        SqlIdentifier tableName = new SqlIdentifier(destinationTable.getTableName(), SqlParserPos.QUOTED_ZERO);
        for (RelDataTypeField field : relationalQueryNode.getRowType().getFieldList()) {
          SqlDataTypeSpec colType = SqlTypeUtil.convertTypeToSpec(field.getType());
          ColumnStrategy strategy = ColumnStrategy.NULLABLE;
          if (!field.getType().isNullable()) {
            strategy = ColumnStrategy.NOT_NULLABLE;
          }
          SqlIdentifier colName = new SqlIdentifier(field.getName(), SqlParserPos.QUOTED_ZERO);
          SqlNode sqlCol = SqlDdlNodes.column(SqlParserPos.ZERO, colName, colType, null, strategy);
          columns.add(sqlCol);
        }
        SqlCreateTable createTableStatement = SqlDdlNodes.createTable(SqlParserPos.ZERO, false,
                                                                      true, tableName, columns, null);
        // Add this to the beginning of the query list
        queries.add(0, createTableStatement.toSqlString(getDialect()).getSql());
      }

      // Serialize the final sink query
      SqlNode sqlQueryNode = converter.visitChild(0, relationalQueryNode).asQueryOrValues();
      SqlBuilder query = new SqlBuilder(getDialect());
      query.append("INSERT INTO ");
      query.identifier(destinationTable.getTableName());
      query.append(' ');
      query.append(sqlQueryNode.toSqlString(getDialect()));
      queries.add(query.toSqlString().getSql());
    }
  }
}
